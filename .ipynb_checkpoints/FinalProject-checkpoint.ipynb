{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14cb37ea",
   "metadata": {},
   "source": [
    "## Import and install all required packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eacc9a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: patchify in /home/mashmallow/.local/lib/python3.8/site-packages (0.2.3)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/mashmallow/.local/lib/python3.8/site-packages (from patchify) (1.22.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /home/mashmallow/.local/lib/python3.8/site-packages (2.8.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imagecodecs in /home/mashmallow/.local/lib/python3.8/site-packages (2022.2.22)\n",
      "Requirement already satisfied: numpy>=1.19.2 in /home/mashmallow/.local/lib/python3.8/site-packages (from imagecodecs) (1.22.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn==0.24.2 in /home/mashmallow/.local/lib/python3.8/site-packages (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/mashmallow/.local/lib/python3.8/site-packages (from scikit-learn==0.24.2) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/mashmallow/.local/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/mashmallow/.local/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/mashmallow/.local/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.22.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install patchify\n",
    "!pip install keras\n",
    "!pip install imagecodecs\n",
    "!pip install -U scikit-learn==0.24.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a2c79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from patchify import patchify\n",
    "import tifffile as tiff\n",
    "import os\n",
    "import re\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354725d",
   "metadata": {},
   "source": [
    "## Multi Unet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98680f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 13:50:13.970122: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:\n",
      "2022-03-15 13:50:13.970191: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "\n",
    "def multi_unet_model(n_classes=5, IMG_HEIGHT=128, IMG_WIDTH=128, IMG_CHANNELS=1):\n",
    "\n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if we normalize our inputs beforehand\n",
    "    s = inputs\n",
    "\n",
    "    #Contraction path\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "    c1 = Dropout(0.1)(c1)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = Dropout(0.1)(c2)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "     \n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "     \n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "     \n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    \n",
    "    #Expansive path \n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "     \n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "     \n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = Dropout(0.1)(c8)\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "     \n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = Dropout(0.1)(c9)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "     \n",
    "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n",
    "     \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    #NOTE: Compile the model in the main program to make it easy to test with various loss functions\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c0aaa",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ff3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = []\n",
    "for subdir, dirs, files in os.walk('./data/train'):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".tiff\"):\n",
    "            \n",
    "            train_filenames.append(filepath)\n",
    "            \n",
    "train_filenames.sort(key=lambda f: int(re.sub('\\D', '', f)))    \n",
    "train_filenames = np.array(train_filenames)\n",
    "\n",
    "all_img_patches = []\n",
    "\n",
    "for img_file in train_filenames:  \n",
    "    large_image = tiff.imread(img_file)\n",
    "    patches_img = patchify(large_image, (128, 128, 3), step=128)\n",
    "    \n",
    "    for i in range(patches_img.shape[0]):\n",
    "        for j in range(patches_img.shape[1]):\n",
    "            \n",
    "            single_patch_img = patches_img[i,j, 0, :, :, :]\n",
    "            single_patch_img = (single_patch_img.astype('float32')) / 255.\n",
    "            \n",
    "            all_img_patches.append(single_patch_img)\n",
    "\n",
    "train_images = np.array(all_img_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b15b42eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask_filenames = []\n",
    "for subdir, dirs, files in os.walk('./data/train_mask'):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".tiff\"):\n",
    "            train_mask_filenames.append(filepath)\n",
    "            \n",
    "train_mask_filenames.sort(key=lambda f: int(re.sub('\\D', '', f)))    \n",
    "train_mask_filenames = np.array(train_mask_filenames)          \n",
    "\n",
    "all_mask_patches = []\n",
    "\n",
    "for img_file in train_mask_filenames:  \n",
    "     \n",
    "    large_mask = tiff.imread(img_file)  \n",
    "    patches_mask = patchify(large_mask, (128, 128), step=128)  #Step=256 for 256 patches means no overlap\n",
    "    \n",
    "\n",
    "    for i in range(patches_mask.shape[0]):\n",
    "        for j in range(patches_mask.shape[1]):\n",
    "            single_patch_mask = patches_mask[i,j,:,:]\n",
    "            all_mask_patches.append(single_patch_mask)\n",
    "\n",
    "train_masks = np.array(all_mask_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f842097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3480, 128, 128, 3)\n",
      "(3480, 128, 128)\n",
      "Pixel values in the mask are:  [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_masks.shape)\n",
    "print(\"Pixel values in the mask are: \", np.unique(train_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40475bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3480, 128, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mashmallow/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode labels... but multi dim array so need to flatten, encode and reshape\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "print(train_masks.shape)\n",
    "n, h, w = train_masks.shape\n",
    "\n",
    "train_masks_reshaped = train_masks.reshape(-1,1)\n",
    "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
    "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "\n",
    "np.unique(train_masks_encoded_original_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370fd10",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e63744db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filenames = []\n",
    "for subdir, dirs, files in os.walk('./data/test'):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".tiff\"):\n",
    "            test_filenames.append(filepath)\n",
    "            \n",
    "test_filenames.sort(key=lambda f: int(re.sub('\\D', '', f)))    \n",
    "test_filenames = np.array(test_filenames)\n",
    "\n",
    "all_img_patches = []\n",
    "\n",
    "for img_file in test_filenames:  \n",
    "    large_image = tiff.imread(img_file)\n",
    "    #large_image = cv2.imread(img_path, 0)    \n",
    "    patches_img = patchify(large_image, (128, 128, 3), step=128)\n",
    "    \n",
    "    for i in range(patches_img.shape[0]):\n",
    "        for j in range(patches_img.shape[1]):\n",
    "            \n",
    "            single_patch_img = patches_img[i,j, 0, :, :, :]\n",
    "            single_patch_img = (single_patch_img.astype('float32')) / 255.\n",
    "            \n",
    "            all_img_patches.append(single_patch_img)\n",
    "\n",
    "test_images = np.array(all_img_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28926634",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask_filenames = []\n",
    "for subdir, dirs, files in os.walk('./data/test_mask'):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".tiff\"):\n",
    "            test_mask_filenames.append(filepath)\n",
    "            \n",
    "test_mask_filenames.sort(key=lambda f: int(re.sub('\\D', '', f)))    \n",
    "test_mask_filenames = np.array(test_mask_filenames)          \n",
    "\n",
    "all_mask_patches = []\n",
    "\n",
    "for img_file in test_mask_filenames:  \n",
    "    large_mask = tiff.imread(img_file)   \n",
    "    #large_image = cv2.imread(img_path, 0)    \n",
    "    patches_mask = patchify(large_mask, (128, 128), step=128)  #Step=256 for 256 patches means no overlap\n",
    "    \n",
    "\n",
    "    for i in range(patches_mask.shape[0]):\n",
    "        for j in range(patches_mask.shape[1]):\n",
    "            single_patch_mask = patches_mask[i,j,:,:]\n",
    "            all_mask_patches.append(single_patch_mask)\n",
    "\n",
    "test_masks = np.array(all_mask_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad16f566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2280, 128, 128, 3)\n",
      "(2280, 128, 128)\n",
      "Pixel values in the mask are:  [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)\n",
    "print(test_masks.shape)\n",
    "print(\"Pixel values in the mask are: \", np.unique(test_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4949e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3480, 128, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode labels... but multi dim array so need to flatten, encode and reshape\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "print(train_masks.shape)\n",
    "n, h, w = train_masks.shape\n",
    "\n",
    "train_masks_reshaped = train_masks.reshape(-1,1)\n",
    "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
    "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "\n",
    "np.unique(train_masks_encoded_original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c4faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c0583c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3480, 128, 128, 3)\n",
      "(3480, 128, 128, 3)\n",
      "(3480, 128, 128, 3)\n",
      "(3480, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import normalize\n",
    "\n",
    "print(train_images.shape)\n",
    "#train_images = np.expand_dims(train_images, axis=3)\n",
    "print(train_images.shape)\n",
    "\n",
    "train_images = normalize(train_images, axis=1)\n",
    "print(train_images.shape)\n",
    "\n",
    "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n",
    "\n",
    "print(train_masks_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cff679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class values in the dataset are ...  [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "#Create a subset of data for quick testing\n",
    "#Picking 10% for testing and remaining for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1, X_test, y1, y_test = train_test_split(train_images, train_masks_input, test_size = 0.10, random_state = 0)\n",
    "\n",
    "#Further split training data t a smaller subset for quick testing of models\n",
    "X_train, X_do_not_use, y_train, y_do_not_use = train_test_split(X1, y1, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"Class values in the dataset are ... \", np.unique(y_train))  # 0 is the background/few unlabeled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f72dbb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2505, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ae97ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Sanity check, view few mages\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "image_number = random.randint(0, len(X_train))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.reshape(X_train[image_number], (128, 128,3)), cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.reshape(y_train[image_number], (128, 128)), cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b966a604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2505, 128, 128, 5)\n",
      "(348, 128, 128, 5)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "n_classes = 5\n",
    "train_masks_cat = to_categorical(y_train, num_classes=n_classes)\n",
    "y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
    "print(y_train_cat.shape)\n",
    "\n",
    "test_masks_cat = to_categorical(y_test, num_classes=n_classes)\n",
    "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))\n",
    "print(y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f3b2c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights are...: [ 0.22325477  6.66665342 59.673897    3.93736819  9.99224859]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight ='balanced',\n",
    "                                                 classes = np.unique(train_masks_reshaped_encoded),\n",
    "                                                 y = train_masks_reshaped_encoded)\n",
    "\n",
    "print(\"Class weights are...:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b7a272c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2505, 128, 128, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 13:50:32.333718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-15 13:50:32.334807: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:\n",
      "2022-03-15 13:50:32.335361: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:\n",
      "2022-03-15 13:50:32.335661: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:\n",
      "2022-03-15 13:50:32.366193: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:\n",
      "2022-03-15 13:50:32.366671: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:\n",
      "2022-03-15 13:50:32.367004: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:\n",
      "2022-03-15 13:50:32.367052: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-03-15 13:50:32.369485: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 128, 128, 16  448         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128, 128, 16  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 128, 128, 16  2320        ['dropout[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 64, 64, 16)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 64, 64, 32)   4640        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64, 64, 32)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 64, 64, 32)   9248        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 64)   18496       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 32, 32, 64)   0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 64)   36928       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)  0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 16, 16, 128)  73856       ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 16, 16, 128)  0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 16, 128)  147584      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 128)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 8, 8, 256)    295168      ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 8, 8, 256)    0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 8, 8, 256)    590080      ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 16, 16, 128)  131200     ['conv2d_9[0][0]']               \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 16, 16, 256)  0           ['conv2d_transpose[0][0]',       \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 16, 16, 128)  295040      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 16, 16, 128)  0           ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 16, 16, 128)  147584      ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 32, 32, 64)  32832       ['conv2d_11[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32, 32, 128)  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 64)   73792       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 32, 32, 64)   0           ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 64)   36928       ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 64, 64, 32)  8224        ['conv2d_13[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 64, 64, 64)   0           ['conv2d_transpose_2[0][0]',     \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 64, 64, 32)   18464       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 64, 64, 32)   0           ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 64, 64, 32)   9248        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_transpose_3 (Conv2DTran  (None, 128, 128, 16  2064       ['conv2d_15[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 128, 128, 32  0           ['conv2d_transpose_3[0][0]',     \n",
      "                                )                                 'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 128, 128, 16  4624        ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128, 128, 16  0           ['conv2d_16[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 128, 128, 16  2320        ['dropout_8[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 128, 128, 5)  85          ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,941,173\n",
      "Trainable params: 1,941,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMG_HEIGHT = X_train.shape[1]\n",
    "IMG_WIDTH  = X_train.shape[2]\n",
    "IMG_CHANNELS = X_train.shape[3]\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "def get_model():\n",
    "    return multi_unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS)\n",
    "\n",
    "model = get_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1abb2beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "314/314 [==============================] - 172s 548ms/step - loss: 0.3837 - accuracy: 0.9011 - val_loss: 0.3215 - val_accuracy: 0.9067\n",
      "Epoch 2/50\n",
      "314/314 [==============================] - 169s 538ms/step - loss: 0.3063 - accuracy: 0.9045 - val_loss: 0.2588 - val_accuracy: 0.9149\n",
      "Epoch 3/50\n",
      "314/314 [==============================] - 179s 569ms/step - loss: 0.2242 - accuracy: 0.9262 - val_loss: 0.2545 - val_accuracy: 0.9243\n",
      "Epoch 4/50\n",
      "314/314 [==============================] - 194s 619ms/step - loss: 0.1622 - accuracy: 0.9458 - val_loss: 0.1530 - val_accuracy: 0.9514\n",
      "Epoch 5/50\n",
      "314/314 [==============================] - 189s 603ms/step - loss: 0.1433 - accuracy: 0.9544 - val_loss: 0.1334 - val_accuracy: 0.9597\n",
      "Epoch 6/50\n",
      "314/314 [==============================] - 177s 564ms/step - loss: 0.1310 - accuracy: 0.9603 - val_loss: 0.1220 - val_accuracy: 0.9641\n",
      "Epoch 7/50\n",
      "314/314 [==============================] - 172s 548ms/step - loss: 0.1142 - accuracy: 0.9661 - val_loss: 0.1436 - val_accuracy: 0.9590\n",
      "Epoch 8/50\n",
      "314/314 [==============================] - 191s 607ms/step - loss: 0.1101 - accuracy: 0.9668 - val_loss: 0.1222 - val_accuracy: 0.9653\n",
      "Epoch 9/50\n",
      "314/314 [==============================] - 178s 566ms/step - loss: 0.0858 - accuracy: 0.9715 - val_loss: 0.1057 - val_accuracy: 0.9708\n",
      "Epoch 10/50\n",
      "314/314 [==============================] - 173s 550ms/step - loss: 0.0949 - accuracy: 0.9699 - val_loss: 0.1051 - val_accuracy: 0.9701\n",
      "Epoch 11/50\n",
      "314/314 [==============================] - 173s 551ms/step - loss: 0.0758 - accuracy: 0.9750 - val_loss: 0.0922 - val_accuracy: 0.9750\n",
      "Epoch 12/50\n",
      "314/314 [==============================] - 188s 600ms/step - loss: 0.0718 - accuracy: 0.9765 - val_loss: 0.0875 - val_accuracy: 0.9756\n",
      "Epoch 13/50\n",
      "314/314 [==============================] - 177s 565ms/step - loss: 0.0707 - accuracy: 0.9768 - val_loss: 0.0843 - val_accuracy: 0.9751\n",
      "Epoch 14/50\n",
      "314/314 [==============================] - 172s 547ms/step - loss: 0.1034 - accuracy: 0.9691 - val_loss: 0.1119 - val_accuracy: 0.9716\n",
      "Epoch 15/50\n",
      "314/314 [==============================] - 175s 559ms/step - loss: 0.0716 - accuracy: 0.9764 - val_loss: 0.0818 - val_accuracy: 0.9776\n",
      "Epoch 16/50\n",
      "314/314 [==============================] - 173s 550ms/step - loss: 0.0654 - accuracy: 0.9785 - val_loss: 0.0771 - val_accuracy: 0.9788\n",
      "Epoch 17/50\n",
      "314/314 [==============================] - 195s 622ms/step - loss: 0.0645 - accuracy: 0.9786 - val_loss: 0.0733 - val_accuracy: 0.9803\n",
      "Epoch 18/50\n",
      "314/314 [==============================] - 172s 548ms/step - loss: 0.0619 - accuracy: 0.9793 - val_loss: 0.0715 - val_accuracy: 0.9806\n",
      "Epoch 19/50\n",
      "314/314 [==============================] - 168s 536ms/step - loss: 0.0598 - accuracy: 0.9800 - val_loss: 0.0728 - val_accuracy: 0.9803\n",
      "Epoch 20/50\n",
      "314/314 [==============================] - 165s 527ms/step - loss: 0.0584 - accuracy: 0.9805 - val_loss: 0.0727 - val_accuracy: 0.9807\n",
      "Epoch 21/50\n",
      "314/314 [==============================] - 162s 517ms/step - loss: 0.1504 - accuracy: 0.9535 - val_loss: 0.2658 - val_accuracy: 0.9171\n",
      "Epoch 22/50\n",
      "314/314 [==============================] - 163s 520ms/step - loss: 0.1372 - accuracy: 0.9549 - val_loss: 0.1170 - val_accuracy: 0.9693\n",
      "Epoch 23/50\n",
      "314/314 [==============================] - 165s 524ms/step - loss: 0.0867 - accuracy: 0.9719 - val_loss: 0.0938 - val_accuracy: 0.9728\n",
      "Epoch 24/50\n",
      "314/314 [==============================] - 170s 540ms/step - loss: 0.0710 - accuracy: 0.9765 - val_loss: 0.0882 - val_accuracy: 0.9749\n",
      "Epoch 25/50\n",
      "314/314 [==============================] - 167s 533ms/step - loss: 0.0641 - accuracy: 0.9786 - val_loss: 0.0864 - val_accuracy: 0.9755\n",
      "Epoch 26/50\n",
      "314/314 [==============================] - 166s 527ms/step - loss: 0.0927 - accuracy: 0.9696 - val_loss: 0.0771 - val_accuracy: 0.9775\n",
      "Epoch 27/50\n",
      "314/314 [==============================] - 177s 563ms/step - loss: 0.0635 - accuracy: 0.9789 - val_loss: 0.0742 - val_accuracy: 0.9785\n",
      "Epoch 28/50\n",
      "314/314 [==============================] - 171s 544ms/step - loss: 0.0597 - accuracy: 0.9797 - val_loss: 0.0744 - val_accuracy: 0.9790\n",
      "Epoch 29/50\n",
      "314/314 [==============================] - 167s 532ms/step - loss: 0.0578 - accuracy: 0.9802 - val_loss: 0.0729 - val_accuracy: 0.9797\n",
      "Epoch 30/50\n",
      "314/314 [==============================] - 163s 520ms/step - loss: 0.0724 - accuracy: 0.9759 - val_loss: 0.1081 - val_accuracy: 0.9712\n",
      "Epoch 31/50\n",
      "314/314 [==============================] - 162s 515ms/step - loss: 0.0702 - accuracy: 0.9764 - val_loss: 0.0717 - val_accuracy: 0.9793\n",
      "Epoch 32/50\n",
      "314/314 [==============================] - 163s 518ms/step - loss: 0.0647 - accuracy: 0.9786 - val_loss: 0.0698 - val_accuracy: 0.9806\n",
      "Epoch 33/50\n",
      "314/314 [==============================] - 167s 531ms/step - loss: 0.0594 - accuracy: 0.9803 - val_loss: 0.0658 - val_accuracy: 0.9824\n",
      "Epoch 34/50\n",
      "314/314 [==============================] - 167s 531ms/step - loss: 0.0560 - accuracy: 0.9814 - val_loss: 0.0647 - val_accuracy: 0.9830\n",
      "Epoch 35/50\n",
      "314/314 [==============================] - 166s 528ms/step - loss: 0.0542 - accuracy: 0.9819 - val_loss: 0.0636 - val_accuracy: 0.9836\n",
      "Epoch 36/50\n",
      "314/314 [==============================] - 169s 539ms/step - loss: 0.0550 - accuracy: 0.9817 - val_loss: 0.0653 - val_accuracy: 0.9831\n",
      "Epoch 37/50\n",
      "314/314 [==============================] - 171s 544ms/step - loss: 0.0530 - accuracy: 0.9821 - val_loss: 0.0681 - val_accuracy: 0.9825\n",
      "Epoch 38/50\n",
      "314/314 [==============================] - 170s 543ms/step - loss: 0.0526 - accuracy: 0.9824 - val_loss: 0.0650 - val_accuracy: 0.9830\n",
      "Epoch 39/50\n",
      "314/314 [==============================] - 172s 549ms/step - loss: 0.0528 - accuracy: 0.9823 - val_loss: 0.0705 - val_accuracy: 0.9821\n",
      "Epoch 40/50\n",
      "314/314 [==============================] - 170s 542ms/step - loss: 0.0558 - accuracy: 0.9815 - val_loss: 0.0656 - val_accuracy: 0.9825\n",
      "Epoch 41/50\n",
      "314/314 [==============================] - 170s 542ms/step - loss: 0.0507 - accuracy: 0.9829 - val_loss: 0.0631 - val_accuracy: 0.9838\n",
      "Epoch 42/50\n",
      "314/314 [==============================] - 168s 536ms/step - loss: 0.0497 - accuracy: 0.9832 - val_loss: 0.0650 - val_accuracy: 0.9832\n",
      "Epoch 43/50\n",
      "314/314 [==============================] - 177s 564ms/step - loss: 0.0484 - accuracy: 0.9836 - val_loss: 0.0626 - val_accuracy: 0.9841\n",
      "Epoch 44/50\n",
      "314/314 [==============================] - 175s 556ms/step - loss: 0.0485 - accuracy: 0.9836 - val_loss: 0.0629 - val_accuracy: 0.9840\n",
      "Epoch 45/50\n",
      "314/314 [==============================] - 171s 544ms/step - loss: 0.0697 - accuracy: 0.9798 - val_loss: 0.1043 - val_accuracy: 0.9715\n",
      "Epoch 46/50\n",
      "314/314 [==============================] - 173s 550ms/step - loss: 0.0611 - accuracy: 0.9802 - val_loss: 0.0699 - val_accuracy: 0.9817\n",
      "Epoch 47/50\n",
      "314/314 [==============================] - 184s 588ms/step - loss: 0.0504 - accuracy: 0.9830 - val_loss: 0.0605 - val_accuracy: 0.9842\n",
      "Epoch 48/50\n",
      "314/314 [==============================] - 181s 575ms/step - loss: 0.0491 - accuracy: 0.9835 - val_loss: 0.0605 - val_accuracy: 0.9844\n",
      "Epoch 49/50\n",
      "314/314 [==============================] - 176s 561ms/step - loss: 0.0476 - accuracy: 0.9839 - val_loss: 0.0609 - val_accuracy: 0.9845\n",
      "Epoch 50/50\n",
      "314/314 [==============================] - 173s 553ms/step - loss: 0.0471 - accuracy: 0.9841 - val_loss: 0.0608 - val_accuracy: 0.9846\n"
     ]
    }
   ],
   "source": [
    "#If starting with pre-trained weights. \n",
    "#model.load_weights('???.hdf5')\n",
    "\n",
    "'''\n",
    "history = model.fit(X_train, y_train_cat, \n",
    "                    batch_size = 8, \n",
    "                    verbose=1, \n",
    "                    epochs=50, \n",
    "                    validation_data=(X_test, y_test_cat), \n",
    "                    #class_weight=class_weights,\n",
    "                    shuffle=False)\n",
    "\n",
    "model.save('test.hdf5')\n",
    "'''\n",
    "\n",
    "#model.save('sandstone_50_epochs_catXentropy_acc_with_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62a1e69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 5s 451ms/step - loss: 0.0608 - accuracy: 0.9846\n",
      "Accuracy is =  98.46368432044983 %\n"
     ]
    }
   ],
   "source": [
    "_, acc = model.evaluate(X_test, y_test_cat)\n",
    "print(\"Accuracy is = \", (acc * 100.0), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7345ac1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#plot the training and validation accuracy and loss at each epoch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(loss) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "#plot the training and validation accuracy and loss at each epoch\n",
    "\n",
    "history = model.load_weights('test.hdf5')\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78c371eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43macc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65c6b648",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "`labels` out of bound\nCondition x < y did not hold.\nFirst 3 elements of x:\n[0 0 0]\nFirst 1 elements of y:\n[4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     10\u001b[0m IOU_keras \u001b[38;5;241m=\u001b[39m MeanIoU(num_classes\u001b[38;5;241m=\u001b[39mn_classes)  \n\u001b[0;32m---> 11\u001b[0m \u001b[43mIOU_keras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_argmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean IoU =\u001b[39m\u001b[38;5;124m\"\u001b[39m, IOU_keras\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#To calculate I0U for each class...\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/metrics_utils.py:70\u001b[0m, in \u001b[0;36mupdate_state_wrapper.<locals>.decorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrying to run metric.update_state in replica context when \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe metric was not created in TPUStrategy scope. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMake sure the keras Metric is created in TPUstrategy scope. \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf_utils\u001b[38;5;241m.\u001b[39mgraph_context_for_symbolic_tensors(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 70\u001b[0m   update_op \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_state_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# update_op will be None in eager execution.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m   metric_obj\u001b[38;5;241m.\u001b[39madd_update(update_op)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/metrics.py:178\u001b[0m, in \u001b[0;36mMetric.__new__.<locals>.update_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m control_status \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    177\u001b[0m ag_update_state \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(obj_update_state, control_status)\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mag_update_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/metrics.py:3071\u001b[0m, in \u001b[0;36m_IoUBase.update_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m   3068\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(sample_weight, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   3070\u001b[0m \u001b[38;5;66;03m# Accumulate the prediction to current confusion matrix.\u001b[39;00m\n\u001b[0;32m-> 3071\u001b[0m current_cm \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3072\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3073\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3074\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3077\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_cm\u001b[38;5;241m.\u001b[39massign_add(current_cm)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/check_ops.py:405\u001b[0m, in \u001b[0;36m_binary_assert\u001b[0;34m(sym, opname, op_func, static_func, x, y, data, summarize, message, name)\u001b[0m\n\u001b[1;32m    402\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m     data \u001b[38;5;241m=\u001b[39m [message] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[0;32m--> 405\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError(\n\u001b[1;32m    406\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    407\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    408\u001b[0m       message\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_pretty_print(d, summarize) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data)))\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# not context.executing_eagerly()\u001b[39;00m\n\u001b[1;32m    411\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: `labels` out of bound\nCondition x < y did not hold.\nFirst 3 elements of x:\n[0 0 0]\nFirst 1 elements of y:\n[4]"
     ]
    }
   ],
   "source": [
    "\n",
    "#IOU\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_argmax=np.argmax(y_pred, axis=3)\n",
    "\n",
    "##################################################\n",
    "\n",
    "#Using built in keras function\n",
    "from keras.metrics import MeanIoU\n",
    "n_classes = 4\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(y_test[:,:,:,0], y_pred_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "\n",
    "#To calculate I0U for each class...\n",
    "values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)\n",
    "print(values)\n",
    "class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[1,0]+ values[2,0]+ values[3,0])\n",
    "class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[0,1]+ values[2,1]+ values[3,1])\n",
    "class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3] + values[0,2]+ values[1,2]+ values[3,2])\n",
    "class4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[0,3]+ values[1,3]+ values[2,3])\n",
    "\n",
    "print(\"IoU for class1 is: \", class1_IoU)\n",
    "print(\"IoU for class2 is: \", class2_IoU)\n",
    "print(\"IoU for class3 is: \", class3_IoU)\n",
    "print(\"IoU for class4 is: \", class4_IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61fc8fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 128, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (32, 128, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/mashmallow/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/mashmallow/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/mashmallow/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/mashmallow/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"/home/mashmallow/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/mashmallow/.local/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"model\" (type Functional).\n    \n    Input 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (32, 128, 1)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(32, 128, 1), dtype=float32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m y_test[test_img_number]\n\u001b[1;32m      7\u001b[0m test_img_norm \u001b[38;5;241m=\u001b[39m test_img[:,:,\u001b[38;5;241m0\u001b[39m][:,:,\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m----> 9\u001b[0m prediction \u001b[38;5;241m=\u001b[39m (\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_img_norm\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     10\u001b[0m predicted_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prediction, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)[\u001b[38;5;241m0\u001b[39m,:,:]\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/mashmallow/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/mashmallow/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/mashmallow/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/mashmallow/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"/home/mashmallow/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/mashmallow/.local/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"model\" (type Functional).\n    \n    Input 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (32, 128, 1)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(32, 128, 1), dtype=float32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "test_img_number = random.randint(0, len(X_test))\n",
    "test_img = X_test[test_img_number]\n",
    "ground_truth = y_test[test_img_number]\n",
    "test_img_norm = test_img[:,:,0][:,:,None]\n",
    "\n",
    "prediction = (model.predict(test_img_input))\n",
    "predicted_img = np.argmax(prediction, axis=3)[0,:,:]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(ground_truth[:,:,0], cmap='jet')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on test image')\n",
    "plt.imshow(predicted_img, cmap='jet')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320dd05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed9d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
